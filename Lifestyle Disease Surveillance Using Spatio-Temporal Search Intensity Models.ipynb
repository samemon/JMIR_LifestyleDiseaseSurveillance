{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LifeStyle Disease Surveillance Using Spatio-Temporal Search Intensity Models\n",
    "(c) 2017 by Shahan Ali Memon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The goal of this research is to predict the prevalence of lifestyle diseases (such as obesity, diabetes, etc.) using Google Trends.\n",
    "\n",
    "### Prerequisite installations\n",
    "-----------------\n",
    "To run this program you'd need the following python libraries\n",
    "1. numpy (pip install numpy)\n",
    "\n",
    "2. sklearn (pip install sklearn)\n",
    "\n",
    "3. matplotlib (optional - pip install matplotlib)\n",
    "\n",
    "4. cPickle (pip install cPickle)\n",
    "\n",
    "### Preprocessing\n",
    "*** Collecting Data and converting to CSV format ***\n",
    "\n",
    "Our first task will be to collect data from Google Trends. You could use pytrends for that. Once the spatial and temporal data is collected and you have applied the spatio-temporal formula mentioned in our paper, you can then use the following code to preprocess the csv file to create numpy arrays.\n",
    "\n",
    "- Hence collect the data from google trends and create a CSV file in the same format as in: Data/Raw/\n",
    "- Once you have the CSV file in the format mentioned, run the preprocess.py file in Data/Code/ as follows:\n",
    "- python preprocess.py <filepath_to_state_list> <filepath_to_csv_file> <directory_path_to_save_numpy_arrays> <comma_separated_training_years> <comma_separated_test_years> <feature_scaling_flag>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program needs 6 arguments\n",
      "Usage: python preprocess.py <fp-s><fp-csv> <fp-o> <train-years> <test-years><scale-flag>\n",
      "fp-s: file path to the list of states\n",
      "fp-csv: file path to the csv file of data\n",
      "fp-o: directory path to save numpy arrays\n",
      "train-years: comma separated years, e.g. 2011,2012\n",
      "test-years: comma separated years, e.g. 2013,2014\n",
      "scale-flag:\n",
      "- 0 for no normalization\n",
      "- 1 for max normalization\n",
      "- 2 for mean normalization\n",
      "- 3 mean normalize test separately from train\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Description: This piece of code is used to preprocess the\n",
    "raw data from csv files to create numpy arrays for train\n",
    "and test.\n",
    "-------------------------------------\n",
    "Author: Shahan A. Memon\n",
    "Advisors: Ingmar Weber, Saquib Razak\n",
    "\n",
    "Copyright: Carnegie Mellon University,\n",
    "Qatar Computing Research Institute 2017\n",
    "--------------------------------------\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "\n",
    "'''\n",
    "Description: This function simply reads a file line by line\n",
    "and creates a python list out of it. For our purposes it reads\n",
    "the states or keywords for which we have data available.\n",
    "Sometimes data for some states or keywords is missing,\n",
    "and so we might not have data available for all the 52 states for example.\n",
    "This is a simple list creator for a file in which data is separated by line\n",
    "Requires:\n",
    "- filename: file which consists of the states/keywords\n",
    "for which data is available\n",
    "Returns: A list of those states\n",
    "'''\n",
    "\n",
    "def readLst(filename):\n",
    "    #Initialize an empty list\n",
    "    lst = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            #Read each line, remove whitespaces and\n",
    "            #append to list\n",
    "            lst.append(line.rstrip())\n",
    "\n",
    "    #return sorted list\n",
    "    return sorted(lst)\n",
    "\n",
    "'''\n",
    "This function reads the training and testing data\n",
    "from a csv file to a numpy array\n",
    "REQUIRES:\n",
    "- states (list of states)\n",
    "- fpcsv (filepath to the csv file)\n",
    "NOTE: the csv file format should match our format\n",
    "PLEASE LOOK into Data/Raw/ to see the file format\n",
    "of our csv files\n",
    "- trn_years (list of training years)\n",
    "- tst_years (list of testing years)\n",
    "- scale (whether to scale the data or not and how)\n",
    "RETURNS:\n",
    "A tuple of (features,x_train,y_train,x_test,y_test)\n",
    "'''\n",
    "\n",
    "def readCSV(states,fpcsv,trn_years,tst_years,scale):\n",
    "    #Read entire data as a string array \n",
    "    all_data = np.genfromtxt(fpcsv,\n",
    "                             dtype=None,\n",
    "                             delimiter=',')\n",
    "    '''\n",
    "    Now we read the labels which are supposed to be in the 3rd col\n",
    "    '''\n",
    "    Y = all_data[:,[2]].flatten()[1:].astype(np.float)\n",
    "    X = all_data[1:,3:].astype(np.float)\n",
    "    if(scale == 1):\n",
    "        X = X / X.max(axis=0)\n",
    "    features = all_data[[0],3:].flatten()\n",
    "    '''\n",
    "    Now that we have X and Y, let's divide them into years\n",
    "    #This code is assuming that years in the csv file are\n",
    "    in ascending order\n",
    "    '''\n",
    "    year_to_arrayX = {}\n",
    "    year_to_arrayY = {}\n",
    "    numberOfYears = len(X[:,[0]])/len(states)\n",
    "    assert(numberOfYears == len(X[:,[0]])*1.0/len(states))\n",
    "    all_years = [2011+i for i in range(numberOfYears)]\n",
    "    assert(set(trn_years) <= set(all_years))\n",
    "    assert(set(tst_years) <= set(all_years))\n",
    "    for i in range(numberOfYears):\n",
    "        year_to_arrayX[all_years[i]] = X[i*len(states):(i+1)*len(states)]\n",
    "        year_to_arrayY[all_years[i]] = Y[i*len(states):(i+1)*len(states)]\n",
    "    #Random initialization\n",
    "    x_train = year_to_arrayX[trn_years[0]]\n",
    "    y_train = year_to_arrayY[trn_years[0]]\n",
    "    for y in trn_years[1:]:\n",
    "        x_train = np.concatenate((x_train,year_to_arrayX\n",
    "                                  [y]), axis=0)\n",
    "        y_train = np.concatenate((y_train,year_to_arrayY\n",
    "                                  [y]), axis=0)\n",
    "    #Random initialization\n",
    "    x_test = year_to_arrayX[tst_years[0]]\n",
    "    y_test = year_to_arrayY[tst_years[0]]\n",
    "    for y in tst_years[1:]:\n",
    "        x_test = np.concatenate((x_test,year_to_arrayX\n",
    "                                  [y]), axis=0)\n",
    "        y_test = np.concatenate((y_test,year_to_arrayY\n",
    "                                  [y]), axis=0)\n",
    "    if(scale == 2):\n",
    "        scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "    if(scale == 3):\n",
    "        x_train = preprocessing.scale(x_train)\n",
    "        x_test = preprocessing.scale(x_test)\n",
    "    \n",
    "    return (x_train,y_train,x_test,y_test,features)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    argv = sys.argv[1:]\n",
    "    if(len(argv)== 6):\n",
    "        assert(len(argv) == 6)\n",
    "        fps = str(argv[0])\n",
    "        fpcsv = str(argv[1])\n",
    "        fpo = str(argv[2])\n",
    "        trn_years = []\n",
    "        tst_years = []\n",
    "        scaleFlag = 1\n",
    "        #Checking if file paths and dirs exist\n",
    "        if(os.path.isdir(fpo) and os.path.isfile(fps) and\n",
    "           os.path.isfile(fpcsv)):\n",
    "            #list of train and test years\n",
    "            try:\n",
    "                trn_years =  map(int,str(argv[3]).split(\",\"))\n",
    "                tst_years =  map(int,str(argv[4]).split(\",\"))\n",
    "            except:\n",
    "                print(\"Error reading training/testing years\")\n",
    "            try:\n",
    "                scaleFlag = int(argv[5])\n",
    "                assert(scaleFlag in [0,1,2,3])\n",
    "            except:\n",
    "                print(\"invalid scaling choice\")\n",
    "            states = readLst(fps)\n",
    "            #readCSV will return a tuple of 4 numpy arrays\n",
    "            x_train,y_train,x_test,y_test,features = readCSV(states,\n",
    "                                                    fpcsv,\n",
    "                                                    sorted(trn_years),\n",
    "                                                    sorted(tst_years),\n",
    "                                                    scaleFlag)\n",
    "            #Saving the numpy arrays\n",
    "            try:\n",
    "                np.save(fpo+\"/\"+\"x_train\", x_train)\n",
    "                np.save(fpo+\"/\"+\"y_train\", y_train)\n",
    "                np.save(fpo+\"/\"+\"x_test\", x_test)\n",
    "                np.save(fpo+\"/\"+\"y_test\", y_test)\n",
    "                np.save(fpo+\"/\"+\"features\",features)\n",
    "                print(\"Saved Arrays successfully in the folder:\"+fpo)\n",
    "            except:\n",
    "                print(\"Error saving arrays\")\n",
    "        else:\n",
    "            print(\"File/Directory Path Error\")\n",
    "        \n",
    "    else:\n",
    "        assert(len(argv) != 6)\n",
    "        print(\"This program needs 6 arguments\\n\"\\\n",
    "              \"Usage: python preprocess.py <fp-s>\"\\\n",
    "              \"<fp-csv> <fp-o> <train-years> <test-years>\"\\\n",
    "              \"<scale-flag>\\n\"\\\n",
    "              \"fp-s: file path to the list of states\\n\"\\\n",
    "              \"fp-csv: file path to the csv file of data\\n\"\\\n",
    "              \"fp-o: directory path to save numpy arrays\\n\"\\\n",
    "              \"train-years: comma separated years, e.g. 2011,2012\\n\"\\\n",
    "              \"test-years: comma separated years, e.g. 2013,2014\\n\"\\\n",
    "              \"scale-flag:\\n\"\\\n",
    "              \"- 0 for no normalization\\n\"\\\n",
    "              \"- 1 for max normalization\\n\"\\\n",
    "              \"- 2 for mean normalization\\n\"\\\n",
    "              \"- 3 mean normalize test separately from train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "After Preprocessing, you would then need to train the model using the numpy arrays created via the program above.\n",
    "To do that you would have to run the program below as follows:\n",
    "\n",
    "python train.py <path_to_x_train.npy> <path_to_y_train.npy> <no_of_folds_of_CV> <loss> <fname_for_model.pkl>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: \n",
      "python train.py <path to x_train.npy><path to y_train.npy> <no of folds of CV> <loss> <fname>\n",
      "where loss = 0 if MAE, 1 if MSE and 2 if r2\n",
      "fname is path+name+.pkl for model to be saved\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Description: This piece of code is train the model\n",
    "and save those models for testing\n",
    "-------------------------------------\n",
    "Author: Shahan A. Memon\n",
    "Advisors: Ingmar Weber, Saquib Razak\n",
    "\n",
    "Copyright: Carnegie Mellon University,\n",
    "Qatar Computing Research Institute 2017\n",
    "--------------------------------------\n",
    "'''\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import cPickle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "Description: Given numpy array x_train and y_train, this\n",
    "function will train a linear regression model\n",
    "REQUIRES:\n",
    "- regularization: flag which will tell whether we should regularize or not\n",
    "- loss function: This can be MAE,MSE,R\n",
    "- cross_validation: flag which will tell if we should do cross-validation\n",
    "- folds: number of folds for cross-validation\n",
    "- x_train: a loaded numpy design matrix\n",
    "- y_train: a loaded numpy label array\n",
    "RETURNS:\n",
    "This function basically returns a lasso regression model to be saved for later\n",
    "and it also returns scores to be used for cross-validation\n",
    "'''\n",
    "\n",
    "def train_LASSOmodel(x_train,y_train,loss,regularization=True,\n",
    "                cross_validation=True,folds=10,\n",
    "                ):\n",
    "    if(loss == 0):\n",
    "        loss = 'neg_mean_absolute_error'\n",
    "    elif(loss == 1):\n",
    "        loss = 'neg_mean_squared_error'\n",
    "    elif(loss == 2):\n",
    "        loss = 'r2'\n",
    "    model = linear_model.Lasso(random_state=0)\n",
    "    #First we need to find a regularization hyper-parameter\n",
    "    alphas = np.arange(0.001,1,0.001)\n",
    "    scores = list()\n",
    "    scores_std = list()\n",
    "    counter = 0\n",
    "    for alpha in alphas:\n",
    "        model.alpha = alpha\n",
    "        this_scores = cross_val_score(model, x_train, y_train,\n",
    "                                      cv = folds, n_jobs = 1,\n",
    "                                      scoring = loss)\n",
    "        scores.append(np.mean(this_scores))\n",
    "        scores_std.append(np.std(this_scores))\n",
    "        counter += 1\n",
    "    opt = max(scores)\n",
    "    print(scores)\n",
    "    \n",
    "    indexLst = [i for i,j in enumerate(scores) if j == opt]\n",
    "    alphaIndex = indexLst[0]\n",
    "    optAlpha = alphas[alphaIndex]\n",
    "    print(\"Optimal Alpha:\"+str(optAlpha))\n",
    "    print (\"score == \" + str(max(scores)))\n",
    "    #So now that we have figured out the alpha\n",
    "    model = linear_model.Lasso(alpha=optAlpha)\n",
    "    model.fit(x_train,y_train)\n",
    "    return model    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    argv = sys.argv[1:]\n",
    "    if(len(argv) == 5):\n",
    "        x_train = np.load(argv[0])\n",
    "        y_train = np.load(argv[1])\n",
    "        folds = int(argv[2])\n",
    "        loss = int(argv[3])\n",
    "        fname = str(argv[4])\n",
    "        model = train_LASSOmodel(x_train,y_train,loss,regularization=True,\n",
    "                cross_validation=True,folds=10)\n",
    "        with open(fname,'wb') as fid:\n",
    "            cPickle.dump(model,fid)\n",
    "        '/Users/samemon/desktop/JMIR_Lifestyle_Disease_Surveillance/Train/Models/train_model.pkl'\n",
    "    else:\n",
    "        print(\"Usage: \\npython train.py <path to x_train.npy>\"\\\n",
    "              \"<path to y_train.npy> <no of folds of CV> <loss> <fname>\\n\"\\\n",
    "              \"where loss = 0 if MAE, 1 if MSE and 2 if r2\\n\"\\\n",
    "              \"fname is path+name+.pkl for model to be saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Once the models have been learnt, you can test the models in terms of their MSE, SMAPE and R correlation as follows:\n",
    "python test.py <fp-test_x> <fp-test_y> <fp-model>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: <fp-test_x> <fp-test_y> <fp-model>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Description: This piece of code is to test the model\n",
    "on a provided test set\n",
    "-------------------------------------\n",
    "Author: Shahan A. Memon\n",
    "Advisors: Ingmar Weber, Saquib Razak\n",
    "\n",
    "Copyright: Carnegie Mellon University,\n",
    "Qatar Computing Research Institute 2017\n",
    "--------------------------------------\n",
    "'''\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import cPickle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def findMSE(true,pred):\n",
    "    return np.mean((true-pred)**2)\n",
    "\n",
    "def findCorr(true,pred):\n",
    "    return np.corrcoef(true,pred)[[0],[1]][0]\n",
    "\n",
    "def findSMAPE(true,pred):\n",
    "    SMAPE = 0\n",
    "    ratio = []\n",
    "    for index in range(len(true)):\n",
    "        sub = abs(true[index]-pred[index])\n",
    "        avg = (true[index]+pred[index])/2.0\n",
    "        ratio.append(sub*1.0/avg)\n",
    "    SMAPE = sum(ratio)/len(true)*100.0\n",
    "    return SMAPE\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    argv = sys.argv[1:]\n",
    "    if(len(argv) == 3):\n",
    "        x_test = np.load(argv[0])\n",
    "        y_test = np.load(argv[1])\n",
    "        model = linear_model.Lasso(random_state=0)\n",
    "        with open(str(argv[2]),'rb') as fid:\n",
    "            model = cPickle.load(fid)\n",
    "        preds = np.array(model.predict(x_test))\n",
    "        MSE = findMSE(y_test,preds)\n",
    "        R = findCorr(y_test,preds)\n",
    "        SMAPE = findSMAPE(y_test,preds)\n",
    "        print(\"MSE ====== :\" + str(MSE))\n",
    "        print(\"R ====== :\" + str(R))\n",
    "        print(\"SMAPE ====== :\" + str(SMAPE))\n",
    "        halfPreds = np.split(preds,2)\n",
    "        halfTrues = np.split(y_test,2)\n",
    "        MSE = (findMSE(halfTrues[0],halfPreds[0]) +\n",
    "               findMSE(halfTrues[1],halfPreds[1]))/2.0\n",
    "        R = (findCorr(halfTrues[0],halfPreds[0]) +\n",
    "               findCorr(halfTrues[1],halfPreds[1]))/2.0\n",
    "        SMAPE = (findSMAPE(halfTrues[0],halfPreds[0]) +\n",
    "               findSMAPE(halfTrues[1],halfPreds[1]))/2.0\n",
    "        print(\"1/2MSE ====== :\" + str(MSE))\n",
    "        print(\"1/2R ====== :\" + str(R))\n",
    "        print(\"1/2SMAPE ====== :\" + str(SMAPE))\n",
    "        \n",
    "    else:\n",
    "        print(\"Usage: <fp-test_x> <fp-test_y> <fp-model>\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
